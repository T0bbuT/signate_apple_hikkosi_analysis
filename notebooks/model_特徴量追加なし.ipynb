{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブックの概要\n",
    "- 特徴量は入力データそのまま\n",
    "- モデルはlightGBM\n",
    "- パラメータチューニングなし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAのまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目的変数yが増加傾向のため、年毎の分布が右にシフトしている。\n",
    "    - to do: 年毎の目的変数yの分布を揃える必要がある。  \n",
    "2. 目的変数yは繁忙期(3月、4月)とそれ以外の時期で分布が異なる\n",
    "3. 休業日close = 1は目的変数y = 0のため、休業日を分離した。\n",
    "4. 目的変数y = 0の分布の右側の裾野が広いので対数変換で裾野を狭めるべし\n",
    "5. 法人対応clientを2014年度から始めた\n",
    "6. 法人が絡む特殊な引越しを行う場合(close = 1)、目的変数の平均値が約9増える\n",
    "7. 料金区分(price_amとprice_pm)は、繁忙期に高い価格を設定している\n",
    "8. 2010年の料金区分が全て欠測値のため削除するべし\n",
    "9. 午後の料金区分は午前よりも安い傾向にある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "pl.Config.set_tbl_cols(100)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import date\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import jpholiday\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込みから前処理まで\n",
    "\n",
    "# データを読み込む\n",
    "train = pl.read_csv(\"../data/input/train.csv\")\n",
    "test = pl.read_csv(\"../data/input/test.csv\")\n",
    "sample_submit = pl.read_csv(\"../data/input/sample_submit.csv\", has_header = False)\n",
    "\n",
    "# IDと日時を分散する\n",
    "train = train.insert_column(0, train[\"datetime\"].alias(\"id\")).with_columns(pl.col(\"datetime\").str.strptime(dtype = pl.Date))\n",
    "test = test.insert_column(0, test[\"datetime\"].alias(\"id\")).with_columns(pl.col(\"datetime\").str.strptime(dtype = pl.Date))\n",
    "\n",
    "# 休業日(close = 1)(と、お盆だけど休業していなくて結局引っ越し数0だった日)を分離する\n",
    "train = train.filter(\n",
    "    (pl.col(\"close\") != 1)\n",
    "    & (pl.col(\"id\") != \"2010-08-18\")\n",
    "    & (pl.col(\"id\") != \"2011-08-14\")\n",
    ")\n",
    "test_close = test.filter(pl.col(\"close\") == 1)[[\"id\"]]\n",
    "test_close = test_close.with_columns(pl.Series(\"y\", [0.0] * len(test_close)))\n",
    "test = test.filter(pl.col(\"close\") != 1)\n",
    "\n",
    "# 目的変数を対数変換する\n",
    "train = train.insert_column(3, train[\"y\"].log().alias(\"y_ln\"))\n",
    "\n",
    "# closeをカラムごと削除\n",
    "train = train.drop(\"close\")\n",
    "test = test.drop(\"close\")\n",
    "\n",
    "# 2010年は料金区分に関する情報が欠損しているので学習から削除\n",
    "train = train.filter(pl.col(\"datetime\") >= date(2011, 1, 1))\n",
    "\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prophetによるベースライン作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年を経るごとにyが上昇していくため、yの分布がどんどん右にシフトしていく問題に対処したい\n",
    "# そこで、prophetによる時系列予測によってyのトレンドを予測し、yのトレンドを除去する\n",
    "\n",
    "# polarsのdatetime型をpandasのdatetime型に変換する必要がある\n",
    "# 訓練データ\n",
    "train_pandas = (\n",
    "    train\n",
    "    .select([\"datetime\", \"y_ln\"])\n",
    "    .rename({\"datetime\": \"ds\", \"y_ln\": \"y\"})\n",
    "    .to_pandas()\n",
    ")\n",
    "# テストデータ\n",
    "test_pandas = (\n",
    "    test\n",
    "    .select([\"datetime\"])\n",
    "    .rename({\"datetime\": \"ds\"})\n",
    "    .to_pandas()\n",
    ")\n",
    "# 学習。超お手軽！\n",
    "model = Prophet()\n",
    "model.fit(train_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.predict(test_pandas).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>prophetの予測内容について</summary>\n",
    "\n",
    "`prophet`が予測した結果には、以下のような列が含まれています。それぞれの列は、時系列予測モデルである`prophet`が生成する予測値やその関連情報を表しています。\n",
    "\n",
    "各列の説明  \n",
    "\n",
    "1. **`ds`**  \n",
    "   - 日付や時刻を表す列です。`prophet`では、この列を時系列データのインデックスとして使用します。予測対象の時間軸を示します。\n",
    "\n",
    "2. **`trend`**  \n",
    "   - モデルが予測した全体的なトレンド（長期的な変化）を表します。データの全体的な増加や減少の傾向を示します。\n",
    "\n",
    "3. **`yhat_lower` / `yhat_upper`**  \n",
    "   - 予測値（`yhat`）の信頼区間の下限と上限を表します。これらは、予測の不確実性を示し、予測値がこの範囲内に収まる可能性が高いことを意味します。\n",
    "\n",
    "4. **`trend_lower` / `trend_upper`**  \n",
    "   - トレンド（`trend`）の信頼区間の下限と上限を表します。トレンドの予測に対する不確実性を示します。\n",
    "\n",
    "5. **`additive_terms`**  \n",
    "   - トレンドに加算される要素の合計値を表します。これには、季節性や休日効果などが含まれます。\n",
    "\n",
    "6. **`additive_terms_lower` / `additive_terms_upper`**  \n",
    "   - 加算要素（`additive_terms`）の信頼区間の下限と上限を表します。\n",
    "\n",
    "7. **`weekly`**  \n",
    "   - 週ごとの季節性を表します。データに週単位の周期的なパターンがある場合、その影響を示します。\n",
    "\n",
    "8. **`weekly_lower` / `weekly_upper`**  \n",
    "   - 週ごとの季節性（`weekly`）の信頼区間の下限と上限を表します。\n",
    "\n",
    "9. **`yearly`**  \n",
    "   - 年ごとの季節性を表します。データに年単位の周期的なパターンがある場合、その影響を示します。\n",
    "\n",
    "10. **`yearly_lower` / `yearly_upper`**  \n",
    "    - 年ごとの季節性（`yearly`）の信頼区間の下限と上限を表します。\n",
    "\n",
    "11. **`multiplicative_terms`**  \n",
    "    - トレンドに乗算される要素の合計値を表します。`prophet`が乗算的な季節性を使用している場合に生成されます。\n",
    "\n",
    "12. **`multiplicative_terms_lower` / `multiplicative_terms_upper`**  \n",
    "    - 乗算要素（`multiplicative_terms`）の信頼区間の下限と上限を表します。\n",
    "\n",
    "13. **`yhat`**  \n",
    "    - モデルが予測した値（予測結果）を表します。この値は、トレンドと加算要素、乗算要素を組み合わせた最終的な予測値です。\n",
    "\n",
    "補足  \n",
    "これらの列は、`prophet`が予測結果を詳細に分解して提供するためのものです。これにより、予測値だけでなく、トレンドや季節性、信頼区間など、予測に影響を与える要素を個別に分析することができます。\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prophetによる予測結果を可視化\n",
    "# 訓練データ\n",
    "forecast_train = model.predict(train_pandas)\n",
    "fig = model.plot(forecast_train)\n",
    "\n",
    "del forecast_train\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手い感じにトレンドを捉えている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定値の算出\n",
    "forecast_train = pl.Series(\"y_ln_prophet\", model.predict(train_pandas)[\"yhat\"].values)\n",
    "forecast_test = pl.Series(\"y_ln_prophet\", model.predict(test_pandas)[\"yhat\"].values)\n",
    "\n",
    "# 推定値をDataFrameにまとめる\n",
    "train = train.insert_column(4, forecast_train)\n",
    "test = test.insert_column(3, forecast_test)\n",
    "\n",
    "# 対数変換した目的変数とProphetによる予測値の差分を計算する\n",
    "train = train.insert_column(5,\n",
    "                            (train[\"y_ln\"] - train[\"y_ln_prophet\"]).alias(\"y_ln_difference\")\n",
    ")\n",
    "\n",
    "# 不要なDataFrameを削除\n",
    "del train_pandas\n",
    "del test_pandas\n",
    "\n",
    "print(train.head(5))\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 様子を可視化\n",
    "train = train.with_columns(pl.col(\"datetime\").dt.year().alias(\"year\"))\n",
    "test = test.with_columns(pl.col(\"datetime\").dt.year().alias(\"year\"))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=2,\n",
    "    height_ratios=[1, 1], width_ratios=[2, 1],\n",
    "    figsize=(15, 8)\n",
    ")\n",
    "\n",
    "g_time_train = sns.lineplot(data = train, x=\"datetime\", y=\"y_ln\", hue=\"year\",palette=\"deep\", ax=axes[0, 0])\n",
    "g_time_prophet_train = sns.lineplot(data = train, x=\"datetime\", y=\"y_ln_prophet\", color=\"0.1\", ax=axes[0, 0])\n",
    "g_time_prophet_train = sns.lineplot(data = test, x=\"datetime\", y=\"y_ln_prophet\", color=\"0.1\", ax=axes[0, 0])\n",
    "g_hist_train = sns.histplot(data = train, x=\"y_ln\", hue=\"year\", palette=\"deep\", ax=axes[0, 1])\n",
    "\n",
    "g_time_diff_train = sns.scatterplot(data = train, x=\"datetime\", y=\"y_ln_difference\", hue=\"year\", palette=\"deep\", marker=\"+\", ax=axes[1, 0])\n",
    "g_hist_diff_train = sns.histplot(data = train, x=\"y_ln_difference\", hue=\"year\", palette=\"deep\", ax=axes[1, 1])\n",
    "\n",
    "# 調整\n",
    "plt.setp(g_time_train.get_xticklabels(), rotation = 90)\n",
    "plt.setp(g_time_train, ylim = (0, 5))\n",
    "plt.setp(g_time_diff_train.get_xticklabels(), rotation = 90)\n",
    "plt.setp(g_time_diff_train, ylim = (-2.5, 2.5))\n",
    "\n",
    "plt.suptitle(\"引越し数 y の推移と、時系列モデルの予測値との差の分布\")\n",
    "plt.tight_layout(rect = [0, 0, 1, 0.96])# 余白の調整  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prophetによる予測との差分を取ることで、うまい具合に年ごとの上昇傾向を排除することができた(年ごとの分布を揃えることができた)  \n",
    "つまり、テストデータ(2016-04-01~2017-03-31)と訓練データ(2011-01-01 ~ 2016-03-31)における目的変数の確率分布が等しいとみなせるようになった  \n",
    "これにより、LGBMなどの強力な予測精度が安定することが期待される  \n",
    "\n",
    "最終的な提出データは、  \n",
    "1. y_ln_differenceをLGBM等適当なモデルに予測させて、\n",
    "2. それをprohetによるy_lnのベースラインに足す。\n",
    "3. 対数変換の逆変換によってy_lnをyに戻す\n",
    "4. 退避させていたclose=1のデータを戻す\n",
    "5. 完成\n",
    "\n",
    "といった感じ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機械学習ライブラリと親和性の高いPandasに変換する\n",
    "train_pandas = train.to_pandas().set_index(\"id\")\n",
    "\n",
    "# 目的変数と特徴量\n",
    "target_column = \"y_ln_difference\"\n",
    "feature_columns = [\"client\", \"price_am\", \"price_pm\"]\n",
    "\n",
    "# 今回は時系列予測なので、訓練データと評価データを分割する際に、時系列の順番を考慮する必要がある\n",
    "# 今回は、2015年1月1日を境に訓練データと評価データを分割する\n",
    "# 訓練データと検証データを期間で分割\n",
    "train_temp = train_pandas[train_pandas[\"datetime\"] < \"2015-01-01\"]\n",
    "valid_temp = train_pandas[train_pandas[\"datetime\"] >= \"2015-01-01\"]\n",
    "\n",
    "# 説明変数と目的変数を分割\n",
    "X_train, y_train = train_temp[feature_columns], train_temp[target_column]\n",
    "X_valid, y_valid = valid_temp[feature_columns], valid_temp[target_column]\n",
    "\n",
    "# 学習\n",
    "model = GradientBoostingRegressor(random_state = 1192).fit(X_train, y_train)\n",
    "\n",
    "# 予測(モデルの予測値をベースラインであるy_ln_prophetに足した後、対数変換を逆変換)\n",
    "y_pred_train = np.exp(model.predict(X_train) + train_temp[\"y_ln_prophet\"])\n",
    "y_pred_valid = np.exp(model.predict(X_valid) + valid_temp[\"y_ln_prophet\"])\n",
    "\n",
    "# 予測精度の確認\n",
    "print(\"訓練データの評価関数:\", np.round(mean_absolute_error(train_temp[\"y\"], y_pred_train), decimals = 7))# 小数第7位はコンペスコアの有効数字\n",
    "print(\"評価データの評価関数:\", np.round(mean_absolute_error(valid_temp[\"y\"], y_pred_valid), decimals = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルによる予測\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "g_time_train = sns.lineplot(data = train_temp, x=\"datetime\", y=model.predict(X_train), ax=ax)\n",
    "g_time_train = sns.lineplot(data = valid_temp, x=\"datetime\", y=model.predict(X_valid), ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "何故か-0.2より小さい値の予測が殆どない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=3, ncols=2,\n",
    "    height_ratios=[1, 1, 1], width_ratios=[2, 1],\n",
    "    figsize=(15, 12)\n",
    ")\n",
    "\n",
    "g_time_train = sns.lineplot(data = train_pandas, x=\"datetime\", y=\"y\", ax=axes[0, 0])\n",
    "g_predict_train = sns.lineplot(data = train_temp, x=\"datetime\", y=y_pred_train, ax=axes[0, 0])\n",
    "g_predict_valid = sns.lineplot(data = valid_temp, x=\"datetime\", y=y_pred_valid, ax=axes[0, 0])\n",
    "\n",
    "g_time_hist_train = sns.histplot(data = train_pandas, x=\"y\", ax=axes[0, 1])\n",
    "g_predict_hist_train = sns.histplot(data = train_temp, x=y_pred_train, ax=axes[0, 1])\n",
    "g_predict_hist_valid = sns.histplot(data = valid_temp, x=y_pred_valid, ax=axes[0, 1])\n",
    "\n",
    "# 残差の確認\n",
    "g_diff_train = sns.scatterplot(data = train_temp, x=\"datetime\", y=train_temp[\"y\"] - y_pred_train, marker = \"+\", ax=axes[1, 0])\n",
    "g_diff_valid = sns.scatterplot(data = valid_temp, x=\"datetime\", y=valid_temp[\"y\"] - y_pred_valid, marker = \"+\", ax=axes[1, 0])\n",
    "axes[1, 0].set_ylabel(\"予測と実際の残差\")\n",
    "\n",
    "g_hist_diff_train = sns.histplot(data = train_temp, x=train_temp[\"y\"] - y_pred_train, bins=50, ax=axes[1, 1])\n",
    "g_hist_diff_valid = sns.histplot(data = valid_temp, x=valid_temp[\"y\"] - y_pred_valid, bins=50, ax=axes[1, 1])\n",
    "\n",
    "# 特徴量重要度\n",
    "g_importance = sns.barplot(x=model.feature_importances_, y=feature_columns, ax=axes[2, 0])\n",
    "g_importance.set_title(\"特徴量重要度\")\n",
    "\n",
    "axes[2, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "del train_temp, valid_temp\n",
    "del X_train, y_train, X_valid, y_valid\n",
    "del y_pred_train, y_pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上昇傾向は上手く捉えられている(prophetのおかげかな)  \n",
    "実際、残差の分布については何とか揃えられている  \n",
    "引っ越し数yが低い場所が上手く予測できていない。何だか、下限が一つの曲線で抑えられてしまっている？  \n",
    "改めて、やっぱ特徴量が少なすぎる。もっといろいろ増やそう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提出データ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出前に、全ての訓練データを使って学習させておく\n",
    "X, y = train_pandas[feature_columns], train_pandas[target_column]\n",
    "model = GradientBoostingRegressor(random_state = 1192).fit(X, y)\n",
    "\n",
    "# 機械学習ライブラリと親和性の高いPandasに変換する\n",
    "test_pandas = test.to_pandas().set_index(\"id\")\n",
    "\n",
    "# 説明変数の分割\n",
    "X_test = test_pandas[feature_columns]\n",
    "\n",
    "# 予測\n",
    "y_pred_test = np.exp(model.predict(X_test) + test_pandas[\"y_ln_prophet\"])\n",
    "\n",
    "# 提出用DataFrame\n",
    "submit = pl.DataFrame({\n",
    "    \"id\": X_test.index.values,\n",
    "    \"y\": y_pred_test.values\n",
    "})\n",
    "\n",
    "# 退避していた休業日のデータを結合\n",
    "submit = pl.concat(\n",
    "    [submit, test_close],\n",
    "    how = \"vertical_relaxed\"\n",
    ").sort(\"id\")\n",
    "\n",
    "# 提出ファイルを保存する\n",
    "# submit.write_csv(\"../data/output/submit_original_features_gbt_default_prm.csv\", include_header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対する予測の可視化\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1, ncols=2,\n",
    "    height_ratios=[1], width_ratios=[2, 1],\n",
    "    figsize=(15, 4)\n",
    ")\n",
    "\n",
    "g_time_train = sns.lineplot(data = train_pandas, x=\"datetime\", y=\"y\", ax=axes[0])\n",
    "g_time_test = sns.lineplot(data = test_pandas, x=\"datetime\", y=y_pred_test, ax=axes[0])\n",
    "\n",
    "g_hist_train = sns.histplot(data = train_pandas, x=\"y\", ax=axes[1])\n",
    "g_hist_test = sns.histplot(data = test_pandas, x=y_pred_test, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うーん…。やはり引っ越し数が低いところの予測が全然できていない。改善が必要だろう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考サイト  \n",
    "[コンペサイト アップル 引越し需要予測](https://signate.jp/competitions/269/data)  \n",
    "[SIGNATE SOTA アップル 引越し需要予測 備忘録](https://zenn.dev/tremendous1192/articles/ea6e73359ee764)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データからあまりに離れた期間の予測は精度が低くなると思う\n",
    "  - 精々1年後程度？そのため、新しい学習データを適宜ちゃんと入れてあげる必要あり\n",
    "- 何故引っ越し数が低いところが上手く予測できなかったのかは結局よくわからん\n",
    "- ラグ特徴量も入れてみたいな。◯◯日前のyの値とかさ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 退避。今回は時系列予測なのでこの分割は不適切なはず\n",
    "\n",
    "# # 今回はy, y_ln_prophetといった元のカラムも一緒に分割したいので、train_test_split関数は使わない\n",
    "# # 訓練データと評価データに分割するマスク\n",
    "# np.random.seed(1192)\n",
    "# train_mask_1 = np.random.choice([True, False], size = len(train_pandas), p = [0.75, 0.25])\n",
    "\n",
    "# # 説明変数、目的変数の分割\n",
    "# # y_train, y_validと書いてはいるが、これは「引っ越し数y」のことではなく「学習時の目的変数としてのy」を表しており\n",
    "# # つまりy_ln_differenceのことであることに注意\n",
    "# train_temp, valid_temp = train_pandas[train_mask_1], train_pandas[~train_mask_1]\n",
    "# X_train, y_train = train_temp[feature_columns], train_temp[target_column] \n",
    "# X_valid, y_valid = valid_temp[feature_columns], valid_temp[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# htmlに変換したものを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "base_name = \"model_特徴量追加なし\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(\"html\")\n",
    "output_file = f\"{base_name}_{timestamp}\"\n",
    "\n",
    "!jupyter nbconvert --to html EDA.ipynb --output \"{output_file}\" --output-dir \"{output_dir}\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb",
   "notebook_metadata_filter": "kernelspec,jupytext"
  },
  "kernelspec": {
   "display_name": "venv_apple_hikkosi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
